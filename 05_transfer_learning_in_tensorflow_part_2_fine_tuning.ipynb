{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e2dfd9-d45a-48f1-8420-8883e0c632dc",
   "metadata": {},
   "source": [
    "# Transfer Learning with Tensorflow part 2: Fine Tuning\n",
    "\n",
    "In the previous notebook, we covered transfer learning feature extraction. Now it's time to learn a new kind of tranfer learning: fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6b1cf5-1876-47f8-a42d-1585038ad8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 20:04:23.831288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754766263.848906    7390 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754766263.854175    7390 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754766263.866303    7390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754766263.866349    7390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754766263.866351    7390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754766263.866353    7390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 20:04:23.870646: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  9 20:04:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.64.05              Driver Version: 575.64.05      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080 ...    Off |   00000000:09:00.0  On |                  N/A |\n",
      "|  0%   38C    P5             11W /  320W |   14708MiB /  16376MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1757      G   /usr/lib/Xorg                           378MiB |\n",
      "|    0   N/A  N/A            1831      G   /usr/bin/gnome-shell                    159MiB |\n",
      "|    0   N/A  N/A            2022      G   /usr/bin/ckb-next                         4MiB |\n",
      "|    0   N/A  N/A            2468      G   /opt/brave-bin/brave                      3MiB |\n",
      "|    0   N/A  N/A            2512      G   ...d37d6279f4d2ad8a627889ce81d26        102MiB |\n",
      "|    0   N/A  N/A            3662      G   /usr/bin/alacritty                       10MiB |\n",
      "|    0   N/A  N/A            4282      G   ...share/Steam/ubuntu12_32/steam          4MiB |\n",
      "|    0   N/A  N/A            4499      G   ./steamwebhelper                         16MiB |\n",
      "|    0   N/A  N/A            4536    C+G   ...am/ubuntu12_64/steamwebhelper          9MiB |\n",
      "|    0   N/A  N/A            5772      C   /usr/bin/python                       13724MiB |\n",
      "|    0   N/A  N/A            6677      G   ...ams-for-linux/teams-for-linux        167MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Python 3.13.5\n",
      "/usr/bin/bash: line 1: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "!nvidia-smi\n",
    "tf.config.list_physical_devices\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "!python --version\n",
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2d7d8f-f16c-4ace-892f-c6db3c7acde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd583dc-d852-4222-8764-72cf0e3ac489",
   "metadata": {},
   "source": [
    "## Creating helper function\n",
    "In previous notebooks, we've created a bunch of helper functions, now we could rewrite them all, however, this is tedious.\n",
    "\n",
    "Always a good idea to use helper functions remember the don't repeat yourself rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae90ee3-1423-450d-9be3-dabe5a3b23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install wget\n",
    "#!pip install scikit-learn\n",
    "#!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76cf37f2-3367-4e4b-bb3b-4b6c0dfb94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions we're going to use in the notebook\n",
    "from helper_functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968e93c-8ec6-489b-8503-91d32e6e8821",
   "metadata": {},
   "source": [
    "> **Note** if you're running this notebook in Colab, the runtime may time out.  When the runtime runs out colab will delete the helper function so will need to redownload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20f253-d14b-4abe-b253-a258d71234d4",
   "metadata": {},
   "source": [
    "## Let's get some data\n",
    "\n",
    "This time we're going to see how we can use the pre-trained models within tf.keras.applications and apply them to our own problem(recognizing images of food).\n",
    "\n",
    "link: https://www.tensorflow.org/api_docs/python/tf/keras/applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473c5450-840b-494e-9dbc-810ba2995729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10% of training data of 10 classes of Food101\n",
    "if(not os.path.exists(\"10_food_classes_10_percent.zip\")):\n",
    "    !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
    "    unzip_data(\"10_food_classes_10_percent.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2377ded6-9a74-4afc-a9b1-32d65e2f4076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n"
     ]
    }
   ],
   "source": [
    "# Checkout how many images and sub directories are in our dataset\n",
    "walk_through_dir(\"10_food_classes_10_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b9deca-89fe-4f94-b2f8-f19aae71f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test directory path\n",
    "train_dir = \"10_food_classes_10_percent/train\"\n",
    "test_dir = \"10_food_classes_10_percent/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244af4af-1055-4365-8536-f9591e363a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 750 files belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754766266.588717    7390 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 737 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 SUPER, pci bus id: 0000:09:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "BATCH_SIZE = 32\n",
    "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                           label_mode=\"categorical\",\n",
    "                                                                           batch_size = BATCH_SIZE)\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
    "                                                                          image_size=IMG_SIZE,\n",
    "                                                                          label_mode=\"categorical\",\n",
    "                                                                          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b34622-fd57-4b55-8886-d1cf698653aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_10_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77be171d-9a6c-4f07-a37d-e05abebce8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chicken_curry',\n",
       " 'chicken_wings',\n",
       " 'fried_rice',\n",
       " 'grilled_salmon',\n",
       " 'hamburger',\n",
       " 'ice_cream',\n",
       " 'pizza',\n",
       " 'ramen',\n",
       " 'steak',\n",
       " 'sushi']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_10_percent.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40c97e6-a9b6-437f-b72f-6fd1fecc016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[1.93841843e+02 1.94586731e+02 1.89714279e+02]\n",
      "   [1.95071426e+02 1.95357147e+02 1.90714279e+02]\n",
      "   [1.96586731e+02 1.95357147e+02 1.93433670e+02]\n",
      "   ...\n",
      "   [1.48270218e+02 1.13555794e+02 1.16698738e+02]\n",
      "   [1.49479721e+02 9.57194290e+01 1.08505165e+02]\n",
      "   [1.61214218e+02 9.67549515e+01 1.13984566e+02]]\n",
      "\n",
      "  [[1.97877548e+02 1.95596939e+02 1.92000000e+02]\n",
      "   [1.99933670e+02 1.95862244e+02 1.92862244e+02]\n",
      "   [2.00341827e+02 1.95642868e+02 1.93357147e+02]\n",
      "   ...\n",
      "   [1.32535751e+02 9.71631470e+01 1.03066315e+02]\n",
      "   [1.51821579e+02 1.01617409e+02 1.13765404e+02]\n",
      "   [1.65301041e+02 1.06040771e+02 1.23897911e+02]]\n",
      "\n",
      "  [[2.01571426e+02 1.92642853e+02 1.91214279e+02]\n",
      "   [2.02301025e+02 1.93372452e+02 1.91943878e+02]\n",
      "   [2.02785721e+02 1.93168365e+02 1.92428574e+02]\n",
      "   ...\n",
      "   [1.47841919e+02 1.16933662e+02 1.22673485e+02]\n",
      "   [1.57678558e+02 1.17663200e+02 1.28892838e+02]\n",
      "   [1.48295731e+02 1.02290619e+02 1.17504898e+02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.94428589e+02 1.74428589e+02 1.73428589e+02]\n",
      "   [1.95000000e+02 1.75000000e+02 1.74000000e+02]\n",
      "   [1.94785706e+02 1.74785706e+02 1.73785706e+02]\n",
      "   ...\n",
      "   [1.51392822e+02 1.25255074e+02 1.53872406e+02]\n",
      "   [1.54045868e+02 1.30117310e+02 1.56117310e+02]\n",
      "   [1.56729507e+02 1.34959122e+02 1.57882584e+02]]\n",
      "\n",
      "  [[1.95071411e+02 1.75071411e+02 1.74071411e+02]\n",
      "   [1.93209152e+02 1.73209152e+02 1.72209152e+02]\n",
      "   [1.93270386e+02 1.73270386e+02 1.72270386e+02]\n",
      "   ...\n",
      "   [1.45081696e+02 1.18438896e+02 1.45653168e+02]\n",
      "   [1.50005219e+02 1.26938911e+02 1.52938919e+02]\n",
      "   [1.53811157e+02 1.34673431e+02 1.56719345e+02]]\n",
      "\n",
      "  [[1.89071442e+02 1.69071442e+02 1.68071442e+02]\n",
      "   [1.89668381e+02 1.69668381e+02 1.68668381e+02]\n",
      "   [1.92214279e+02 1.72214279e+02 1.71214279e+02]\n",
      "   ...\n",
      "   [9.91382294e+01 7.29851837e+01 9.95566559e+01]\n",
      "   [1.22245186e+02 1.00245186e+02 1.23245186e+02]\n",
      "   [1.43408371e+02 1.24408371e+02 1.46408371e+02]]]\n",
      "\n",
      "\n",
      " [[[1.17285713e+02 1.95153065e+01 1.58265314e+01]\n",
      "   [1.19000000e+02 2.13316326e+01 1.73367348e+01]\n",
      "   [1.14076530e+02 1.95714302e+01 1.25816326e+01]\n",
      "   ...\n",
      "   [1.28780533e+02 1.31566269e+02 1.10352005e+02]\n",
      "   [1.29933685e+02 1.32933685e+02 1.13933685e+02]\n",
      "   [1.29765244e+02 1.32765244e+02 1.15765244e+02]]\n",
      "\n",
      "  [[1.17668373e+02 2.06683674e+01 1.38571434e+01]\n",
      "   [1.17000000e+02 2.29336739e+01 1.49234695e+01]\n",
      "   [1.15056122e+02 2.10561218e+01 1.10561228e+01]\n",
      "   ...\n",
      "   [1.32173462e+02 1.35173462e+02 1.14173470e+02]\n",
      "   [1.32500000e+02 1.35500000e+02 1.16500000e+02]\n",
      "   [1.30780563e+02 1.33780563e+02 1.16780571e+02]]\n",
      "\n",
      "  [[1.16428574e+02 2.17857132e+01 1.04285717e+01]\n",
      "   [1.16214287e+02 2.22142868e+01 1.06428576e+01]\n",
      "   [1.15785713e+02 2.17857132e+01 1.02142859e+01]\n",
      "   ...\n",
      "   [1.40714233e+02 1.43714233e+02 1.22714241e+02]\n",
      "   [1.41525543e+02 1.44525543e+02 1.25525543e+02]\n",
      "   [1.44469406e+02 1.47469406e+02 1.30469406e+02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.37142868e+01 1.67142849e+01 6.71428585e+00]\n",
      "   [3.19438782e+01 1.49438782e+01 4.94387817e+00]\n",
      "   [3.35714188e+01 1.65714188e+01 6.57141972e+00]\n",
      "   ...\n",
      "   [1.60617386e+02 1.37831680e+02 1.24045975e+02]\n",
      "   [1.58887741e+02 1.36102036e+02 1.22316322e+02]\n",
      "   [1.52494812e+02 1.29709106e+02 1.15923401e+02]]\n",
      "\n",
      "  [[3.58367729e+01 1.88367748e+01 8.83677483e+00]\n",
      "   [3.48520927e+01 1.78520947e+01 7.85209417e+00]\n",
      "   [3.46428909e+01 1.76428909e+01 7.64289188e+00]\n",
      "   ...\n",
      "   [1.66556229e+02 1.44556229e+02 1.31556229e+02]\n",
      "   [1.59994934e+02 1.37994934e+02 1.24994926e+02]\n",
      "   [1.57357056e+02 1.35357056e+02 1.22357056e+02]]\n",
      "\n",
      "  [[2.90561237e+01 1.20561237e+01 2.05612421e+00]\n",
      "   [3.11173458e+01 1.41173458e+01 4.11734629e+00]\n",
      "   [2.74285717e+01 1.04285717e+01 4.28571701e-01]\n",
      "   ...\n",
      "   [1.70010223e+02 1.51010223e+02 1.37010223e+02]\n",
      "   [1.60714294e+02 1.41714294e+02 1.27714294e+02]\n",
      "   [1.61856995e+02 1.39856995e+02 1.26856995e+02]]]\n",
      "\n",
      "\n",
      " [[[2.46642853e+02 2.46642853e+02 2.48642853e+02]\n",
      "   [2.48331635e+02 2.48331635e+02 2.50331635e+02]\n",
      "   [2.48923477e+02 2.48923477e+02 2.48923477e+02]\n",
      "   ...\n",
      "   [1.70050945e+01 1.90816174e+01 1.55000439e+01]\n",
      "   [6.89285049e+01 7.62091675e+01 8.59490509e+01]\n",
      "   [6.66067047e+01 7.97802887e+01 1.04035484e+02]]\n",
      "\n",
      "  [[2.49188782e+02 2.49188782e+02 2.51188782e+02]\n",
      "   [2.47994904e+02 2.47994904e+02 2.49994904e+02]\n",
      "   [2.48285706e+02 2.48285706e+02 2.48285706e+02]\n",
      "   ...\n",
      "   [1.92295685e+01 2.34540424e+01 2.12296562e+01]\n",
      "   [6.19694710e+01 7.21838226e+01 8.47400589e+01]\n",
      "   [4.54787903e+01 6.30043755e+01 9.22392044e+01]]\n",
      "\n",
      "  [[2.51066330e+02 2.50066330e+02 2.54785721e+02]\n",
      "   [2.49270401e+02 2.49255096e+02 2.51301025e+02]\n",
      "   [2.48214279e+02 2.48214279e+02 2.48551025e+02]\n",
      "   ...\n",
      "   [1.69795570e+01 2.48571434e+01 2.64592896e+01]\n",
      "   [3.19848995e+01 4.99135246e+01 7.14289474e+01]\n",
      "   [2.61222782e+01 5.32753906e+01 9.36888428e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.71428795e+01 5.17143517e+01 5.89286156e+01]\n",
      "   [3.41275520e+01 4.86990242e+01 5.59132881e+01]\n",
      "   [3.32142639e+01 4.78775635e+01 5.46173401e+01]\n",
      "   ...\n",
      "   [2.03550674e+02 1.89239365e+02 1.84453629e+02]\n",
      "   [1.89372147e+02 1.74443588e+02 1.67300705e+02]\n",
      "   [1.72366898e+02 1.58366898e+02 1.47366898e+02]]\n",
      "\n",
      "  [[3.24745102e+01 4.96173935e+01 5.65459518e+01]\n",
      "   [3.11428699e+01 4.81428680e+01 5.51428680e+01]\n",
      "   [3.25561066e+01 4.91275368e+01 5.56989632e+01]\n",
      "   ...\n",
      "   [1.76902679e+02 1.63474152e+02 1.58116943e+02]\n",
      "   [1.52764709e+02 1.39764709e+02 1.31693268e+02]\n",
      "   [1.10713379e+02 9.77133789e+01 8.87133789e+01]]\n",
      "\n",
      "  [[3.53571434e+01 5.63571434e+01 6.13571434e+01]\n",
      "   [3.21428566e+01 5.11428566e+01 5.71428566e+01]\n",
      "   [3.13673630e+01 4.85000687e+01 5.47908592e+01]\n",
      "   ...\n",
      "   [1.23717888e+02 1.10289360e+02 1.04932152e+02]\n",
      "   [8.32232666e+01 7.02232666e+01 6.22232628e+01]\n",
      "   [5.35503693e+01 4.35503693e+01 3.35503693e+01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.78693878e+02 1.65693878e+02 2.37693878e+02]\n",
      "   [1.77412308e+02 1.65412308e+02 2.37412308e+02]\n",
      "   [1.83340240e+02 1.71340240e+02 2.43340240e+02]\n",
      "   ...\n",
      "   [7.19897461e+01 5.64895897e+01 7.64179688e+01]\n",
      "   [3.78065262e+01 1.16636410e+01 1.42349911e+01]\n",
      "   [3.82128334e+01 9.21283340e+00 2.49847746e+00]]\n",
      "\n",
      "  [[1.75608734e+02 1.62608734e+02 2.34608734e+02]\n",
      "   [1.74950256e+02 1.61950256e+02 2.33950256e+02]\n",
      "   [1.84265945e+02 1.72265945e+02 2.44265945e+02]\n",
      "   ...\n",
      "   [7.11361465e+01 5.42074661e+01 7.23501053e+01]\n",
      "   [4.03489151e+01 1.42060328e+01 1.67773819e+01]\n",
      "   [4.42846794e+01 1.52846785e+01 8.57032299e+00]]\n",
      "\n",
      "  [[1.74905609e+02 1.61905609e+02 2.33905609e+02]\n",
      "   [1.72725113e+02 1.59725113e+02 2.31725113e+02]\n",
      "   [1.78859695e+02 1.66096298e+02 2.38096298e+02]\n",
      "   ...\n",
      "   [5.87836647e+01 3.98919411e+01 5.72711868e+01]\n",
      "   [3.93963928e+01 1.32535105e+01 1.44071512e+01]\n",
      "   [4.05132217e+01 1.15132217e+01 5.34420013e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.13441162e+02 1.63356659e+02 2.01119461e+02]\n",
      "   [2.24765640e+02 1.73947739e+02 2.08074768e+02]\n",
      "   [2.42282532e+02 1.88933029e+02 2.17022415e+02]\n",
      "   ...\n",
      "   [2.33467300e+02 1.77709900e+02 1.54726303e+02]\n",
      "   [2.27351105e+02 1.69114471e+02 1.49587738e+02]\n",
      "   [2.24691833e+02 1.63455200e+02 1.46165100e+02]]\n",
      "\n",
      "  [[2.08373718e+02 1.58352997e+02 1.96415161e+02]\n",
      "   [2.12955612e+02 1.62955612e+02 1.99959763e+02]\n",
      "   [2.22722015e+02 1.71560394e+02 2.04048950e+02]\n",
      "   ...\n",
      "   [2.38443008e+02 1.82085800e+02 1.61228745e+02]\n",
      "   [2.31540192e+02 1.72540192e+02 1.54540192e+02]\n",
      "   [2.24533997e+02 1.62533997e+02 1.47533997e+02]]\n",
      "\n",
      "  [[2.08062469e+02 1.56709808e+02 1.90767792e+02]\n",
      "   [2.14668045e+02 1.63315384e+02 1.97373367e+02]\n",
      "   [2.14348175e+02 1.65348175e+02 1.96919601e+02]\n",
      "   ...\n",
      "   [2.29897766e+02 1.73540558e+02 1.53192139e+02]\n",
      "   [2.25385452e+02 1.66385452e+02 1.50385452e+02]\n",
      "   [2.19683701e+02 1.57683701e+02 1.44683701e+02]]]\n",
      "\n",
      "\n",
      " [[[2.42857151e+01 1.12857141e+01 5.28571415e+00]\n",
      "   [2.65000000e+01 1.35000010e+01 7.50000048e+00]\n",
      "   [2.76479588e+01 1.46479588e+01 8.64795876e+00]\n",
      "   ...\n",
      "   [9.98470001e+01 5.48469963e+01 1.41224804e+01]\n",
      "   [1.12500130e+02 7.15715714e+01 2.53572464e+01]\n",
      "   [1.35745224e+02 9.87452240e+01 4.67452202e+01]]\n",
      "\n",
      "  [[2.63316326e+01 1.33316326e+01 7.33163261e+00]\n",
      "   [2.60663261e+01 1.30663261e+01 7.06632614e+00]\n",
      "   [2.65714283e+01 1.35714283e+01 7.57142830e+00]\n",
      "   ...\n",
      "   [9.51836777e+01 5.07143288e+01 1.21989594e+01]\n",
      "   [1.04928703e+02 6.39338036e+01 1.96429615e+01]\n",
      "   [1.32163742e+02 9.45668411e+01 4.43575592e+01]]\n",
      "\n",
      "  [[2.43571434e+01 1.13571424e+01 5.35714293e+00]\n",
      "   [2.37295914e+01 1.07295923e+01 4.72959185e+00]\n",
      "   [2.53571415e+01 1.23571424e+01 6.35714245e+00]\n",
      "   ...\n",
      "   [9.10459366e+01 4.66174088e+01 1.05969439e+01]\n",
      "   [9.77806473e+01 5.58827095e+01 1.41530895e+01]\n",
      "   [1.18000557e+02 7.98628082e+01 3.45770264e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.34642982e+01 3.55255165e+01 2.18112736e+01]\n",
      "   [5.87244110e+01 4.25815506e+01 2.92244530e+01]\n",
      "   [4.92294388e+01 3.77039223e+01 2.36580524e+01]\n",
      "   ...\n",
      "   [8.78573608e+00 3.78573608e+00 7.78573608e+00]\n",
      "   [7.00000000e+00 2.00000000e+00 6.00000000e+00]\n",
      "   [8.35717773e+00 3.35717773e+00 7.35717773e+00]]\n",
      "\n",
      "  [[6.00305252e+01 4.30305252e+01 3.38366661e+01]\n",
      "   [4.92652321e+01 3.51325989e+01 2.61989231e+01]\n",
      "   [4.08163452e+01 3.08469620e+01 2.08316536e+01]\n",
      "   ...\n",
      "   [6.78573608e+00 4.78573608e+00 7.78573608e+00]\n",
      "   [5.00000000e+00 3.00000000e+00 6.00000000e+00]\n",
      "   [6.35717773e+00 4.35717773e+00 7.35717773e+00]]\n",
      "\n",
      "  [[4.43110886e+01 2.73110867e+01 2.03110867e+01]\n",
      "   [4.20204010e+01 2.88775463e+01 2.09489746e+01]\n",
      "   [4.21478958e+01 3.26428223e+01 2.36428223e+01]\n",
      "   ...\n",
      "   [6.78573608e+00 4.78573608e+00 7.78573608e+00]\n",
      "   [5.00000000e+00 3.00000000e+00 6.00000000e+00]\n",
      "   [6.35717773e+00 4.35717773e+00 7.35717773e+00]]]\n",
      "\n",
      "\n",
      " [[[1.00000000e+00 1.00000000e+00 3.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 2.12946439e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   ...\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [1.45111084e-01 1.45111084e-01 1.45111084e-01]]\n",
      "\n",
      "  [[1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   ...\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [1.45111084e-01 1.45111084e-01 1.45111084e-01]]\n",
      "\n",
      "  [[1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   ...\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "   [3.28301668e-01 3.28301668e-01 3.28301668e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00 2.85491085e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 2.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 2.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [1.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      "   [3.28282952e-01 3.28282952e-01 1.83171868e-01]]\n",
      "\n",
      "  [[0.00000000e+00 2.85491085e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 2.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 2.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [1.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      "   [1.45111084e-01 1.45111084e-01 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 2.85491085e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 2.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 2.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [1.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      "   [1.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      "   [1.45111084e-01 1.45111084e-01 0.00000000e+00]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]], shape=(32, 10), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 20:04:27.648125: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# See an example of batch of data\n",
    "for images,labels in train_data_10_percent.take(1):\n",
    "    print(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf63e14-21c5-4591-b830-cbcb67ce4862",
   "metadata": {},
   "source": [
    "## Model 0: building a transfer learning model using the Keras Functional API\n",
    "\n",
    "The sequential API is straight-forward, it runs our layers in sequential order.\n",
    "\n",
    "But the functional API gives us more flexibility in desiging our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872052d1-9a26-4bbc-8a97-fa6b5c5034ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after passing inputs through the base model:(None, 7, 7, 1280)\n",
      "Shape after GlobalAveragePooling2D:(None, 1280)\n",
      "Saving TensorBoard log files to: transfer learning/10_percent_feature_extraction/20250809-200429\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# Create base model\n",
    "efficentnet_b0 = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    classes=10,\n",
    "    classifier_activation='softmax',\n",
    ")\n",
    "\n",
    "efficentnet_b0.trainable = False\n",
    "\n",
    "# create inputs for model\n",
    "inputs = tf.keras.layers.Input(shape=(224,224,3),name=\"input_layer\")\n",
    "# normalize, needed for some architectures\n",
    "# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "x = efficentnet_b0(inputs)\n",
    "print(f\"Shape after passing inputs through the base model:{x.shape}\")\n",
    "# average pool the outputs of base model(aggregate most important information, reduce computational expenses)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "print(f\"Shape after GlobalAveragePooling2D:{x.shape}\")\n",
    "# create output activation layer\n",
    "outputs = tf.keras.layers.Dense(10,activation=\"softmax\",name=\"output_layer\")(x)\n",
    "# combine inputs and outputs into model\n",
    "model_0 = tf.keras.Model(inputs,\n",
    "                   outputs)\n",
    "\n",
    "model_0.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                       loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                       metrics=[\"accuracy\"])\n",
    "history_model_0 = model_0.fit(train_data_10_percent,epochs=5,validation_data=test_data,validation_steps=int(0.25*len(test_data)),callbacks=[create_tensorboard_callback(dir_name=\"transfer learning\",experiment_name=\"10_percent_feature_extraction\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460d880-bea8-43a8-a4c6-ec305187b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62dea3-4e5a-489d-8081-c0619441029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the layers in our base model\n",
    "for layer_number, layer in enumerate(efficentnet_b0.layers):\n",
    "    print(f\"Layer number:{layer_number} layer name:{layer.name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf681d6-c45f-46ba-af03-0809b3751c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of base model\n",
    "efficentnet_b0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a9ab0-000c-4d45-94fa-782f900be854",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885286a4-ebe1-4c82-b14a-f170224c660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out models training curves\n",
    "plt.show(plot_loss_curves(history_model_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1c4ce-fc04-48e3-8d8d-f84d9d455715",
   "metadata": {},
   "source": [
    "## Getting a feature vector from a trained model \n",
    "Let's demonstrate the Global Average Pooling 2d layer...\n",
    "We have a tensor after our model goes through `base+model` of shape (None,7,7,1280).\n",
    "\n",
    "But then when it passes through GlobalAveragePooling2D, it turns into (None,1280)\n",
    "\n",
    "Let's use a similar shaped tensor of (1,4,4,3) and then pass it to GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f09c4-d445-4625-b68e-6c8a520a58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input shape\n",
    "input_shape =(1,4,4,3)\n",
    "#create a random tensor\n",
    "tf.random.set_seed(42)\n",
    "input_tensor=tf.random.normal(input_shape)\n",
    "print(f\"Random input tensor:\\n{input_tensor}\\n\")\n",
    "\n",
    "# Pass the random tensor through a random global average pooling 2D layer\n",
    "global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\n",
    "print(f\"2D global average pooled random tensor:\\n{global_average_pooled_tensor}\\n\")\n",
    "\n",
    "# Check the shape of the different tensors\n",
    "print(f\"Shape of input tensor: {input_tensor.shape}\")\n",
    "print(f\"Shape of global average pooled 2D: {global_average_pooled_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfee266-a798-48e5-b2e8-3d7724b8a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replicate the GlobalAveragePool2D layer\n",
    "tf.reduce_mean(input_tensor,axis=[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86cbbc2-aeac-4d5f-ae33-e337653e8cc9",
   "metadata": {},
   "source": [
    "**Practice** Try to do the same with the above two cells but this time use `GlobalMaxPool2D`... and see what happens\n",
    "\n",
    "**Note** Onen of the reasons feature extraction transfer learning is named how it is is because what often happens is a pretrained model outputs a feature vector - a learned representation of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df934a-31f7-42ac-b8f9-7fbd2085ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max_pool_tensor = tf.keras.layers.GlobalMaxPool2D()(input_tensor) \n",
    "print(global_max_pool_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3613a67-8f88-4e98-b723-51c91fbbb84b",
   "metadata": {},
   "source": [
    "# Running a series of transfer learning experiments\n",
    "\n",
    "We've seen the incredible results transfer learning can get with only 10% of the training data, but how does it go with only 1% of the training data... how about we set up a bunch of experiments to find out:\n",
    "\n",
    "1. `model_1` - use feature extraction transfer learning iwth 1% of the training data with augmentation\n",
    "2. `model_2` - use feature extraction transfer learning with 10% of the training data with data augmentation\n",
    "3. `model_3` - use fine-tuning transfer learning with 10% of the training data which will also use data augmentation\n",
    "4. `model_4`- use fine-tuning transfer learning on 100% of the training data with data augmentation.\n",
    "   \n",
    "**Note**: throughout all experiments we will use the same test dataset, so that we can be consistent in evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d112731-c6ee-41ca-9133-22c9bd6ffaf8",
   "metadata": {},
   "source": [
    "## Getting and preprocessing data for model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98019ce9-65c6-4c81-80b0-a7ac8bd83604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip data\n",
    " \n",
    "if not os.path.exists(\"./10_food_classes_1_percent.zip\"):\n",
    "    !curl --output \"10_food_classes_1_percent.zip\" -X GET \"https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\"\n",
    "    unzip_data(\"./10_food_classes_1_percent.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfdb19-bd1b-4252-bcc2-8c7ad55f1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test dirs\n",
    "train_dir_1_percent = \"10_food_classes_1_percent/train\"\n",
    "test_dir = \"10_food_classes_1_percent/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73731ef3-fbc2-49e9-8458-5ba689380774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many images are we working with\n",
    "walk_through_dir(\"10_food_classes_1_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d6d40-3b2f-487d-ac66-1d38a4c16cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data loaders\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,\n",
    "                                                                          label_mode=\"categorical\",\n",
    "                                                                          image_size=IMG_SIZE,\n",
    "                                                                          batch_size=BATCH_SIZE)\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE,\n",
    "                                                                batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7191eb7-f04d-44d2-aaa3-309d564cfeee",
   "metadata": {},
   "source": [
    "# Adding data augmentation right into the model\n",
    "\n",
    "\n",
    "To add data augmentation right into our models, we can use the layers inside:\n",
    "\n",
    "+ `tf.keras.layers.experimental.preprocessing()`\n",
    "\n",
    "Benefits of data augmentation\n",
    "\n",
    "+ More data - model may be better able to generalize\n",
    "+ preprocessing of images(augmenting them) happens on the GPU which is far faster for this type of problem than the CPU\n",
    "+ Image data augmentation only happens during training so we can still export out model and use it elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab61aef-da7e-4ace-bc3b-0954e8f3913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create data augmentation stage with horizontal flipping, rotations, zooms,etc.\n",
    "    \n",
    "data_augmentation = keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomHeight(0.2),\n",
    "    tf.keras.layers.RandomWidth(0.2),\n",
    "    # tf.keras.layers.Rescaling(1./255) - keep for models like resnet50V2 but for efficientnet it has rescaling built in\n",
    "],name=\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13754d23-7a69-4f32-a412-a56270242064",
   "metadata": {},
   "source": [
    "### visualize our data augmentation layer(and check the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41902399-93f9-4074-8c99-9c7fb802ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a random image and compare it to the augmented version\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import random\n",
    "target_class = random.choice(train_data_1_percent.class_names)\n",
    "target_dir = \"10_food_classes_1_percent/train/\" + target_class\n",
    "random_image = random.choice(os.listdir(target_dir))\n",
    "random_image_path = target_dir + \"/\" + random_image\n",
    "print(target_dir)\n",
    "# read in random image + plot\n",
    "img = mpimg.imread(random_image_path)\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Original random image from class {target_class}\")\n",
    "plt.axis(False)\n",
    "# now let's plot augmented random image\n",
    "augmented_image = data_augmentation(img)\n",
    "plt.figure()\n",
    "plt.show()\n",
    "plt.title(f\"Augmented random image from class {target_class}\")\n",
    "plt.axis(False)\n",
    "plt.imshow(tf.squeeze(augmented_image / 255.))\n",
    "plt.show()\n",
    "print(img)\n",
    "print(random_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f55d2-34ce-43c9-a356-2ae6a7d5e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomHeight(0.2),\n",
    "    tf.keras.layers.RandomWidth(0.2),\n",
    "], name=\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca23eb5-f688-43b7-9945-be550defaf35",
   "metadata": {},
   "source": [
    "## Model 1: Feature extraction on transfer learning on 1% of the data using data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45026ae6-e164-45d7-b5e5-a8a05cc491c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup input shape and base model freezing the base model layers\n",
    "input_shape=(224,224,3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable=False\n",
    "\n",
    "# Crete input layer\n",
    "inputs = layers.Input(shape=input_shape,name=\"input_layer\")\n",
    "print(inputs)\n",
    "# add in data augmentation Sequential model as layer\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# GIve base_model the inputs(after augmentation) & don't train it\n",
    "\n",
    "x = base_model(x,training=False)\n",
    "\n",
    "# Pool output features of the base model\n",
    "\n",
    "x = layers.GlobalAveragePooling2D(name=\"global_avg_pooling_layer\")(x)\n",
    "\n",
    "# Put a dense layer on as the output\n",
    "outputs = layers.Dense(10,activation=\"softmax\",name=\"ouput_layer\")(x)\n",
    "\n",
    "# Make a model using inputs and outpus\n",
    "\n",
    "model_1 = keras.Model(inputs,outputs)\n",
    "\n",
    "# compile the model\n",
    "\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "# Fit the model\n",
    "history_1_percent = model_1.fit(train_data_1_percent,\n",
    "                               epochs=5,\n",
    "                               steps_per_epoch=len(train_data_1_percent),\n",
    "                               validation_data=test_data,\n",
    "                               validation_steps=int(0.25*len(test_data)),\n",
    "                               callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",experiment_name=\"1_percent_data_aug\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13675e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout model 1 summary\n",
    "model_1.summary()\n",
    "history_1_percent.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on full data set\n",
    "results_1_percent_data_aug = model_1.evaluate(test_data)\n",
    "results_1_percent_data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for data augmentation with 1 percent\n",
    "plt.show(plot_loss_curves(history_1_percent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb6802",
   "metadata": {},
   "source": [
    "## Model 2: Feature extraction on transfer learning on 10% of the data using data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692aecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_10_percent = \"./10_food_classes_10_percent/train\"\n",
    "test_dir_10_percent = \"./10_food_classes_10_percent/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,\n",
    "                                                               label_mode=\"categorical\",\n",
    "                                                               image_size=IMG_SIZE)\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir_10_percent,\n",
    "                                                               label_mode=\"categorical\",\n",
    "                                                               image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa6caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many images are in dir\n",
    "walk_through_dir(\"10_food_classes_10_percent/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9369260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomHeight(0.2),\n",
    "    layers.RandomWidth(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomRotation(0.2)\n",
    "    # if using another model rescaling may be needed as efficientNet has rescaling built in\n",
    "])\n",
    "# Setup input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44935ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup input shape and base model freezing the base model layers\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_shape=(224,224,3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable=False\n",
    "\n",
    "# Crete input layer\n",
    "inputs = layers.Input(shape=input_shape,name=\"input_layer\")\n",
    "\n",
    "# add in data augmentation Sequential model as layer\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# Give base_model the inputs(after augmentation) & don't train it\n",
    "x = base_model(x,training=False)\n",
    "\n",
    "# Pool output features of the base model\n",
    "x = layers.GlobalAveragePooling2D(name=\"global_avg_pooling_layer\")(x)\n",
    "\n",
    "# Put a dense layer on as the output\n",
    "outputs = layers.Dense(10,activation=\"softmax\",name=\"ouput_layer\")(x)\n",
    "\n",
    "# Make a model using inputs and outpus\n",
    "model_2 = keras.Model(inputs,outputs)\n",
    "\n",
    "# compile the model\n",
    "\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "# Fit the model\n",
    "history_10_percent = model_2.fit(train_data,\n",
    "                               epochs=5,\n",
    "                               steps_per_epoch=len(train_data),\n",
    "                               validation_data=test_data,\n",
    "                               validation_steps=int(0.25*len(test_data)),\n",
    "                               callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",experiment_name=\"10_percent_data_aug\")])\n",
    "data_augmentation.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71605b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77adc7ce",
   "metadata": {},
   "source": [
    "## Creating a model checkpoint callback\n",
    "\n",
    "The ModelCheckpoint callback intermediately saves our model(full model or just weights) during training.  This is useful so we can pause training and come back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path='./ten_percent_model_checkpoints-weights/checkpoint.weights.h5'\n",
    "\n",
    "# Create a model checkpoint callback to save weights\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=False,\n",
    "    save_freq='epoch', # save every epoch\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_10_percent = model_2.fit(train_data,\n",
    "                               epochs=5,\n",
    "                               steps_per_epoch=len(train_data),\n",
    "                               validation_data=test_data,\n",
    "                               validation_steps=int(0.25*len(test_data)),\n",
    "                               callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",experiment_name=\"10_percent_data_aug\"),model_checkpoint_callback])\n",
    "data_augmentation.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d44bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What were model 0 results\n",
    "model_0.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9933bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model 2 results on all test data \n",
    "results_10_percent_data_aug = model_2.evaluate(test_data)\n",
    "results_10_percent_data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model loss curves\n",
    "plt.show(plot_loss_curves(history_10_percent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf06359",
   "metadata": {},
   "source": [
    "### Loading in checkpointed weights\n",
    "\n",
    "Loadining in checkpointed weights returns a model to a specific checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in saved model weights and evaluate weights\n",
    "model_2.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ae499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model_2 with loaded weights\n",
    "loaded_weights_model_results = model_2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23776d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the results from our previously evaluated model_2 match the loaded weights, everything has worked!\n",
    "results_10_percent_data_aug == loaded_weights_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_weights_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_10_percent_data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if loaded model results are very close to our previous non loaded results(precision issue)\n",
    "\n",
    "import numpy as np\n",
    "np.isclose(np.array(results_10_percent_data_aug),np.array(loaded_weights_model_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(results_10_percent_data_aug) - np.array(loaded_weights_model_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a5247b",
   "metadata": {},
   "source": [
    "## Model 3: Fine-tuning an existing model on 10% of the data\n",
    "\n",
    "**Note:** Fine-tuning usually works best *after* training a feataure extraction model for a few epochs with large amounts of custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6af7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers in loaded model:\n",
    "model_2.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edd08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are layers trainable\n",
    "for layer in model_2.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What layers are in our base_model(EfficientNetB0) and are they trainable?\n",
    "for i,layer in enumerate(model_2.layers[2].layers):\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many trainable variables are in our base model\n",
    "print(len(model_2.layers[2].trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1758b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To begin fine-tuning, let's start by setting the last 10 layers of our base model.trainable =True\n",
    "base_model.trainable = True\n",
    "# Freeze all layers except for last 10\n",
    "for i,layer in enumerate(model_2.layers[2].layers[:-10]):\n",
    "    layer.trainable= False\n",
    "# Recompile (We have to recompile our models after every change we make)\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # When fine-tuning you typically want to lower the learning rate by 10X\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d420380",
   "metadata": {},
   "source": [
    "> **Note:** when using fine-tuning it's best practice to lower your learning rate by some amount.  How much?  This is hyperparameter you can tune.  But a good rule of thumb is 10X but sources may vary.  A good resource for this is the ULMFiT paper for fine-tuning for text classification https://arxiv.org/abs/1801.06146."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which layers are trainable\n",
    "for layer_number, layer in enumerate(model_2.layers[2].layers):\n",
    "    print(layer_number,layer.name,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've unfrozen some of the layers closer to the top, how many trainable variables are there\n",
    "\n",
    "print(len(model_2.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_2.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634efb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune for another 5 epochs\n",
    "initial_epochs = 5\n",
    "fine_tune_epochs = initial_epochs + 5\n",
    "# refit the model(Same as model_2 except with more trainable layers)\n",
    "history_fine_10_percent_data_aug = model_2.fit(train_data_10_percent,\n",
    "                                              epochs=fine_tune_epochs,\n",
    "                                              validation_data=test_data,\n",
    "                                              validation_steps=int(0.25 * len(test_data)),\n",
    "                                              initial_epoch=history_10_percent.epoch[-1], # start training from previous last epoch\n",
    "                                              callbacks = [create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
    "                                                                                       experiment_name=\"10_percent_fine_tune_last_10\")]\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b97a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model (model_3 which is actually model_2 fine-tuned for another 5 epochs)\n",
    "results_fine_tuned_10_percent = model_2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292df6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_10_percent_data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkout the loss curves of our fine_tuned_model\n",
    "\n",
    "plt.show(plot_loss_curves(history_fine_10_percent_data_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa681a",
   "metadata": {},
   "source": [
    "The `plot_loss_curves` function works great with models which have only beeen fit once, however, we want something to compare one series of running `fit()` with another(eg: before and after fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaabe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a funciton\n",
    "def compare_histories(original_history,new_history,initial_epochs=5):\n",
    "    \"\"\"\n",
    "    Compares two Tensorflow History object.s\n",
    "    \"\"\"\n",
    "    # Get original history measurements\n",
    "    acc = original_history.history[\"accuracy\"]\n",
    "    loss = original_history.history[\"loss\"]\n",
    "    \n",
    "    val_acc = original_history.history[\"val_accuracy\"]\n",
    "    val_loss = original_history.history[\"val_loss\"]\n",
    "    \n",
    "    # combine original history\n",
    "    total_acc = acc + new_history.history[\"accuracy\"]\n",
    "    total_loss = loss  + new_history.history[\"loss\"]\n",
    "    \n",
    "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
    "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
    "    \n",
    "    # make plots\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(total_acc, label=\"Training Accuracy\")\n",
    "    plt.plot(total_val_acc, label=\"Val accuracy\")\n",
    "    plt.plot([initial_epochs-1,initial_epochs-1],plt.ylim(), label=\"Start Fine Tuning\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    \n",
    "        # make plots\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(total_loss, label=\"Training Loss\")\n",
    "    plt.plot(total_val_loss, label=\"Val Loss\")\n",
    "    plt.plot([initial_epochs-1,initial_epochs-1],plt.ylim(), label=\"Start Fine Tuning\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show(compare_histories(history_10_percent,history_fine_10_percent_data_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0e5b3-86dd-4f4c-bdf9-0a94d39c4e05",
   "metadata": {},
   "source": [
    "## Model 4:  Fine-tuning and existing model on all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308a0e0-bccf-421f-b11d-191aa4999aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip 10 classes of Food101 with all images\n",
    "if(not os.path.exists(\"10_food_classes_all_data.zip\")):\n",
    "    !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
    "unzip_data(\"10_food_classes_all_data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc65bc4-590a-4a5f-a56b-3a4f118a8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training and test dirs\n",
    "train_dir_all_data = \"10_food_classes_all_data/train\"\n",
    "test_dir = \"10_food_classes_all_data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffbc0b-2d2b-4339-aff4-998e4c7d03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_through_dir(\"10_food_classes_all_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ec910-2e54-439a-8f1d-20336e428734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setupu data inputs\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE=(224,224)\n",
    "train_data_10_classes_full = tf.keras.preprocessing.image_dataset_from_directory(train_dir_all_data,\n",
    "                                                                              label_mode=\"categorical\",\n",
    "                                                                              image_size=IMG_SIZE)\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                             label_mode=\"categorical\",\n",
    "                                                             image_size=IMG_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb11b48-984c-4c96-aa7a-6589d17cb622",
   "metadata": {},
   "source": [
    "The test dataset we've loaded in is the same as what we've been using for previous experiments (all experiments have used the same test dataset).\n",
    "\n",
    "Let's verify this...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7dcf7f-d699-493a-9a52-63090d0b46af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model 2(this is the fine-tuned on 10 percent of data version)\n",
    "model_2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a760cce-8fe2-4d74-96b1-5d766c9cf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fine_tuned_10_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe53f6-6815-482f-a704-37fbb81452ea",
   "metadata": {},
   "source": [
    "To train a fine-tuning model (model_4) we need to revert model_2 back to it's feature extraction weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61386742-ff43-4129-9744-038937c7b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights from checkpoint, that way we can fine-tune from\n",
    "# The same stage the 10 percent data model was fine_tuned from\n",
    "model_2.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243c20e-d306-4c62-a554-db370bea3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets evaluate model 2 now\n",
    "model_2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64662d01-cb0b-410d-8c33-96c35d272c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if our model_2 has been reverted back to feature extraction results\n",
    "results_10_percent_data_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597e003-2eb3-48e5-8fb9-d51928a75b70",
   "metadata": {},
   "source": [
    "Alright, the previous steps might seem quite confusing but all we've done is:\n",
    "\n",
    "1. Trained a feature extraction transfer-learning model for 5 epochs on 10% of the data with data augmentation(model_2) and we've saved the model's weights using `ModelCheckpoint` callback.\n",
    "2. Fine-tuned the same model on the same 10% of the data for a further 5 epochs with the top 10 layers of the base model unfrozen(model_3)\n",
    "3. Saved the resutls and training logs each time.\n",
    "4. Reloaded the model from step 1 to do the same steps as step 2 except this time we're going to use all of the data(model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba515a-f69d-41ee-92b1-4d9a75b6468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which laysers are tunable in the whole model\n",
    "for layer_number, layer in enumerate(model_2.layers):\n",
    "    print(layer_number, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4021b-d8a5-484f-b373-fdd5bf344a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drill into our base_model(efficientnetb0) and see what layers are trainable\n",
    "for layer_number, layer in enumerate(model_2.layers[2].layers):\n",
    "    print(layer_number,layer.name,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b52829-53c3-4a5c-9c63-c23fd392a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd42ef3-ffdc-413c-9477-c045bcf024ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue to train and fine-tune the model to our data(100% of our training data)\n",
    "fine_tune_epochs = initial_epochs + 5\n",
    "history_fine_10_classes_full = model_2.fit(train_data_10_classes_full,\n",
    "                                          epochs=fine_tune_epochs,\n",
    "                                          validation_data=test_data,\n",
    "                                          validation_steps=int(0.25 * len(test_data)),\n",
    "                                          initial_epoch=5,\n",
    "                                           callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
    "                                                                                  experiment_name=\"full_10_classes_fine_tune_last_10\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca32fc-ed8e-4861-a566-90d6a972af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate on all test data\n",
    "results_fine_tune_full_data=model_2.evaluate(test_data)\n",
    "results_fine_tune_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37c3b0-c7c6-4e0f-8590-67c22047d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did fine-tuning go with more data?\n",
    "compare_histories(original_history=history_fine_10_percent_data_aug, new_history=history_fine_10_classes_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa61efe-665b-446a-bad7-e448af0ef8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
